{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting BERT Models (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to interpret Bert models using  `Captum` library. In this particular case study we focus on a fine-tuned Question Answering model on SQUAD dataset using transformers library from Hugging Face: https://huggingface.co/transformers/\n",
    "\n",
    "We show how to use interpretation hooks to examine and better understand embeddings, sub-embeddings, bert, and attention layers. \n",
    "\n",
    "Note: Before running this tutorial, please install `seaborn`, `pandas` and `matplotlib`, `transformers`(from hugging face) python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "# from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerIntegratedGradients, IntegratedGradients, LayerConductance, TokenReferenceBase\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/media/roy/ngillani/school_ratings/models\")\n",
    "from robert_regressor import MeanBertForSequenceRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "# model_path = '<PATH-TO-SAVED-MODEL>'\n",
    "model_path = '/media/roy/ngillani/school_ratings/models/checkpoints/top_level/max_len_30_lr_0.0001/epoch_1_training_loss_0.328397995247_val_loss_0.332628061866.pt'\n",
    "\n",
    "# HERE: https://github.com/huggingface/transformers/issues/2094#issuecomment-563346322\n",
    "# 1) Using hugginface's save_model, can then call from_pretrained where model_path is the directory\n",
    "# 2) did some version of torch.save()\n",
    "    # config = BertConfig.from_pretrained(\"bert-base-cased\", num_labels=3)\n",
    "    # model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", config=config)\n",
    "    # model.load_state_dict(torch.load(\"SAVED_SST_MODEL_DIR/pytorch_model.bin\"))\n",
    "\n",
    "# Version 2\n",
    "config = BertConfig(output_attentions=True)\n",
    "# model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "model = MeanBertForSequenceRegression(config)\n",
    "sys.path.append('/media/roy/ngillani/school_ratings/models')  # model being saved requires bert_regresser.py to be in path\n",
    "state_dict = torch.load(model_path, map_location=torch.device('cpu')).state_dict()  # currently loads BertForSequenceRegression\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Should be loading from model_path, but again, model wasn't saved with huggingface's save...() function\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pickle\n",
    "BATCH_SIZE = 1\n",
    "prepared_data_file = '/media/roy/ngillani/school_ratings/data/tiny_by_school_top_level.p'\n",
    "\n",
    "with open(prepared_data_file, 'rb') as f:\n",
    "     all_input_ids, labels_test_score, attention_masks = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_IND = 0\n",
    "\n",
    "input_ids = torch.LongTensor([all_input_ids['train'][DATA_IND]]).unsqueeze(0) # 1 x num_tokens (probably 512)\n",
    "label_t = torch.tensor([labels_test_score['train'][DATA_IND]])\n",
    "input_mask = torch.tensor([attention_masks['train'][DATA_IND]]).unsqueeze() # 1 x num_tokens (probably 512)\n",
    "\n",
    "print (input_ids.size())\n",
    "\n",
    "preds1 = model(input_ids, attention_mask=input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "token_reference = TokenReferenceBase(reference_token_idx=ref_token_id)\n",
    "ref_input_ids = token_reference.generate_reference(input_ids.size(1), device=device).unsqueeze(0).long()\n",
    "print (ref_input_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_forward_wrapper(input_ids, attention_mask=None, position=0):\n",
    "    return model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_mask = input_mask.long()\n",
    "input_ids = input_ids.long()\n",
    "ref_input_ids = ref_input_ids.long()\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "lig = LayerIntegratedGradients(bert_forward_wrapper, model.bert.embeddings)\n",
    "\n",
    "attributions, conv_delta = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "    additional_forward_args=(input_mask, 0),  return_convergence_delta=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to summarize attributions for each word token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    print(attributions)\n",
    "    print(attributions.size())\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_sum = summarize_attributions(attributions)\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = viz.VisualizationDataRecord(\n",
    "                        attributions_sum,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        attributions_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        conv_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_text(datarecords):\n",
    "    dom = [\"<table width: 100%>\"]\n",
    "    rows = [\n",
    "        \"<th>Attribution Score</th>\"\n",
    "        \"<th>Word Importance</th>\"\n",
    "    ]\n",
    "    for datarecord in datarecords:\n",
    "        rows.append(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    \"<tr>\",\n",
    "                    viz.format_classname(\"{0:.2f}\".format(datarecord.attr_score)),\n",
    "                    viz.format_word_importances(\n",
    "                        datarecord.raw_input, datarecord.word_attributions\n",
    "                    ),\n",
    "                    \"<tr>\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    dom.append(\"\".join(rows))\n",
    "    dom.append(\"</table>\")\n",
    "    display(viz.HTML(\"\".join(dom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_text([vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/bert/visuals_of_start_end_predictions.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
